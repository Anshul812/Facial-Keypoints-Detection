{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import  numpynumpy  as  npnp\n",
    "import  matplotlib.pyplot  as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "from models import Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "# load in color image for face detection\n",
    "image = cv2.imread('images/obamas.jpg')\n",
    "\n",
    "\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# plot the image\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier('detector_architectures\\\\haarcascade_frontalface_default.xml')\n",
    "\n",
    "\n",
    "faces = face_cascade.detectMultiScale(image, 1.2, 2)\n",
    "\n",
    "\n",
    "image_with_detections = image.copy()\n",
    "\n",
    "\n",
    "for (x,y,w,h) in faces:\n",
    "    # draw a rectangle around each detected face\n",
    "    # you may also need to change the width of the rectangle drawn depending on image resolution\n",
    "    cv2.rectangle(image_with_detections,(x,y),(x+w,y+h),(255,0,0),3) \n",
    "\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "\n",
    "plt.imshow(image_with_detections);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = gpu(Net())\n",
    "\n",
    "net.load_state_dict(torch.load('keypoints_model_1.pt'))\n",
    "\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_output(faces, test_outputs):  \n",
    "    batch_size = len(faces)\n",
    "    for i, face in enumerate(faces):\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        ax = plt.subplot(1, batch_size, i+1)\n",
    "\n",
    "        # un-transform the predicted key_pts data\n",
    "        predicted_key_pts = test_outputs[i].data\n",
    "        predicted_key_pts = predicted_key_pts.numpy()\n",
    "        # undo normalization of keypoints  \n",
    "        predicted_key_pts = predicted_key_pts*50.0+100\n",
    "\n",
    "        plt.imshow(face, cmap='gray')\n",
    "        plt.scatter(predicted_key_pts[:, 0], predicted_key_pts[:, 1], s=20, marker='.', c='r')\n",
    "        \n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "image_copy = np.copy(image)\n",
    "images, keypoints = [], []\n",
    "PADDING = 25\n",
    "\n",
    "# loop over the detected faces from your haar cascade\n",
    "for (x,y,w,h) in faces:\n",
    "    \n",
    "    # Select the region of interest that is the face in the image \n",
    "    roi = image_copy[y-PADDING:y+h+PADDING, x-PADDING:x+w+PADDING]\n",
    "    \n",
    "    ## TODO: Convert the face region from RGB to grayscale\n",
    "    roi = cv2.cvtColor(roi, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "    ## TODO: Normalize the grayscale image so that its color range falls in [0,1] instead of [0,255]\n",
    "    roi = (roi / 255.).astype(np.float32)\n",
    "    \n",
    "    ## TODO: Rescale the detected face to be the expected square size for your CNN (224x224, suggested)\n",
    "    roi = cv2.resize(roi, (224, 224))\n",
    "    images.append(roi)\n",
    "    \n",
    "    ## TODO: Reshape the numpy image shape (H x W x C) into a torch image shape (C x H x W)\n",
    "    if len(roi.shape) == 2:\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "    else:\n",
    "        roi = np.rollaxis(roi, 2, 0)\n",
    "    \n",
    "    # Make it a batch of length 1\n",
    "    # If you don't, this happens:\n",
    "    # - https://discuss.pytorch.org/t/expected-stride-to-be-a-single-integer-value-or-a-list-of-1-values-to-match-the-convolution-dimensions-but-got-stride-1-1/17140\n",
    "    # - https://stackoverflow.com/questions/50073358/pytorch-cnn-stride-error\n",
    "    roi = np.expand_dims(roi, axis=0)\n",
    "    \n",
    "    ## TODO: Make facial keypoint predictions using your loaded, trained network \n",
    "    ## wrap each face region in a Variable and perform a forward pass to get the predicted facial keypoints\n",
    "    roi = gpu(torch.from_numpy(roi).type(torch.FloatTensor))\n",
    "    results = net.forward(roi)\n",
    "    \n",
    "    results = results.view(results.size()[0], 68, -1).cpu()\n",
    "    keypoints.append(results[0])\n",
    "    \n",
    "## TODO: Display each detected face and the corresponding keypoints\n",
    "visualize_output(images, keypoints)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
